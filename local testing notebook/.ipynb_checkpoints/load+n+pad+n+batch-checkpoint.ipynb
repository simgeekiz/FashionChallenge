{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-58711fb742a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m \u001b[1;31m# data processing, CSV file I/O (e.g. pd.read_csv)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplications\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mvgg16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minception_v3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresnet50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmobilenet\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Globally-importable utils.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Unknown backend: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_BACKEND\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensor_array_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import json\n",
    "from keras.applications import vgg16, inception_v3, resnet50, mobilenet\n",
    "from keras.preprocessing import image\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageOps\n",
    "import random\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, BatchNormalization, Conv2D, MaxPooling2D, Dropout, Flatten, MaxPool2D\n",
    "from keras import optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "with open('train.json') as json_data:\n",
    "    data = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(data['images'][1002]['imageId'])\n",
    "#data['annotations'][0]['labelId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#padding function\n",
    "\n",
    "\n",
    "def add_padding(img):\n",
    "    desired_size = 600\n",
    "    old_size = img.size  # old_size[0] is in (width, height) format\n",
    "    ratio = float(desired_size)/max(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "\n",
    "    im = img.resize(new_size, Image.ANTIALIAS)\n",
    "    padding = Image.new(\"RGB\", (desired_size, desired_size), color =(0,0,0))\n",
    "    padding.paste(im, ((desired_size-new_size[0])//2, (desired_size-new_size[1])//2))\n",
    "    \n",
    "    \n",
    "    return padding\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#loading images + labels\n",
    "\n",
    "ids = data['images']\n",
    "ys = data['annotations']\n",
    "\n",
    "\n",
    "batches_idx = random.sample(range(0,10000), 10000)\n",
    "val_idx = batches_idx[0:999]\n",
    "\n",
    "def get_batch_idx(batchnr): #1 tm 9 for dataset of 10 000 images and 9 batches + 1 validation set\n",
    "    if(batchnr > 9 or batchnr < 1):\n",
    "        print(\"wrong batchnr!!!!!!!!!\")\n",
    "    else:\n",
    "        return batches_idx[1000*batchnr:1000*batchnr+999]\n",
    "    \n",
    "#228 different labels\n",
    "def label_to_many_hot_encoding(datalist, nr_of_labels):\n",
    "    newdata = np.zeros([len(datalist), nr_of_labels])\n",
    "    for i in range(0, len(datalist)):\n",
    "        for nr in datalist[i]:\n",
    "            newdata[i][int(nr)-1] = 1\n",
    "        \n",
    "    return newdata\n",
    "\n",
    "\n",
    "def load_images(idx):\n",
    "    batchx = []\n",
    "    batchy = []\n",
    "    for i in idx:\n",
    "        imgid = ids[i]\n",
    "        y = ys[i]\n",
    "   # for imgid, y in zip(ids, ys):\n",
    "        try:\n",
    "            img = image.load_img(\"json_images\\{}.jpg\".format(imgid[\"imageId\"]))\n",
    "            x = image.img_to_array(add_padding(img))\n",
    "\n",
    "            batchx.append(x)\n",
    "            batchy.append(y[\"labelId\"])\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "    return batchx,label_to_many_hot_encoding(batchy,228)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    n_classes = 228\n",
    "    data_size = (600,600,1)\n",
    "    #convolution\n",
    "    network = Sequential()\n",
    "    \n",
    "    network.add(MaxPool2D(pool_size=(3,3), input_shape=data_size))\n",
    "   # network.add(Conv2D(20, (100,100), input_shape = data_size))\n",
    "    network.add(Conv2D(10, (30,30)))\n",
    "    network.add(BatchNormalization())\n",
    "    network.add(Activation('relu'))\n",
    "    \n",
    "    network.add(MaxPool2D(pool_size=(2,2)))\n",
    "    network.add(Dropout(0.25))\n",
    "    \n",
    "    network.add(Conv2D(10, (20,20)))\n",
    "    network.add(BatchNormalization())\n",
    "    network.add(Activation('relu'))\n",
    "   \n",
    "    \n",
    "    network.add(MaxPool2D(pool_size=(2,2)))\n",
    "    \n",
    "    network.add(Dropout(0.25))\n",
    "    \n",
    "    network.add(Flatten())\n",
    "    \n",
    "    \n",
    "    network.add(Dense(20))\n",
    "    network.add(BatchNormalization())\n",
    "    network.add(Activation('relu'))\n",
    "    \n",
    "    \n",
    "    network.add(Dropout(0.5))\n",
    "    \n",
    "    network.add(Dense(n_classes, activation='sigmoid'))\n",
    "    \n",
    "    loss = 'binary_crossentropy'\n",
    "    adam = optimizers.Adam(lr = 0.001)\n",
    "    metrics = ['accuracy'] \n",
    "\n",
    "    network.compile(loss=loss, optimizer=adam, metrics=metrics)\n",
    "\n",
    "    return network\n",
    "\n",
    "test = create_model()\n",
    "\n",
    "test.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train in batches\n",
    "\n",
    "#create model here\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "print(\"network created\")\n",
    "\n",
    "#load validation data for validation\n",
    "\n",
    "valx, valy = load_images(val_idx)\n",
    "\n",
    "print(\"validation images loaded\")\n",
    "\n",
    "for batch in range(1,2): #1 tm 9 for dataset of 10 000 images and 9 batches + 1 validation set\n",
    "    print(\"start batch {}\".format(batch))\n",
    "    batch_idx = get_batch_idx(batch)\n",
    "    batchx, batchy = load_images(batch_idx)\n",
    "    print(\"Loaded batch {} with {} datapoints\".format(batch, len(batchx)))\n",
    "    \n",
    "    #train model with batch here\n",
    "    \n",
    "    #model.fit(batchx, batchy)\n",
    "    \n",
    "    #optionally do intermediate validation \n",
    "    results =  model.fit(np.sarray(batchx), batchy) #,validation_data=(valx,valy))\n",
    "    print(\"train accuracy: {}\".format(results.history['acc']))\n",
    "    validation = model.evaluate(np.sarrayvalx, valy)\n",
    "    print(\"validation score: {}\".format(scores[1]))\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "#final validation\n",
    "#model.evaluate(valx, valy)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cell to look at individual images, just for testing, not important :)\n",
    "\n",
    "\n",
    "#img = cv2.imread(\"C:\\Users\\Eireen\\Documents\\ML\\project\\json_images\\9998.jpg\")\n",
    "img = image.load_img(\"json_images\\9968.jpg\")\n",
    "\n",
    "\n",
    "x = image.img_to_array(add_padding(img))\n",
    "#print(\"img type: {} x type: {} x shape: {}\".format(type(img), type(x), x.shape))\n",
    "plt.imshow(x.astype(np.uint8))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#228 different labels\n",
    "def label_to_many_hot_encoding(datalist, nr_of_labels):\n",
    "    newdata = np.zeros([len(datalist), nr_of_labels])\n",
    "    for i in range(0, len(datalist)):\n",
    "        for nr in datalist[i]:\n",
    "            newdata[i][int(nr)-1] = 1\n",
    "        \n",
    "    return newdata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "een = [3, 4]\n",
    "twee = [1, 2]\n",
    "drie = [2, 3]\n",
    "vier = [3, 4, 5]\n",
    "vijf = [2]\n",
    "\n",
    "lijstje = [een, twee, drie, vier, vijf]\n",
    "\n",
    "label_to_many_hot_encoding(valy, 228)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
