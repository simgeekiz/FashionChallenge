{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "\n",
    "from keras.applications.xception import Xception\n",
    "from keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from batch_generator import BatchGenerator, BatchSequence\n",
    "\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = 'C:/Users/Joan/Desktop/Uni/Machine Learning in Practice/LocalFC/ImbalanceTest/images'\n",
    "images_path_train = join(datadir, 'train/')\n",
    "images_path_validation = join(datadir, 'validation/')\n",
    "annotation_path = join(datadir, 'D:/mlip/')\n",
    "\n",
    "# loading labels\n",
    "y_train = np.load(join(annotation_path, 'multilabel_train.npy'))\n",
    "y_validation = np.load(join(annotation_path, 'multilabel_validation.npy'))\n",
    "\n",
    "# load the generator\n",
    "training_gen = BatchGenerator(input_dir=images_path_train, y=y_train, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 2997s 3s/step - loss: 0.0754\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 2381s 2s/step - loss: 0.0677\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 2174s 2s/step - loss: 0.0666\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 2082s 2s/step - loss: 0.0658\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 2070s 2s/step - loss: 0.0653\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 2067s 2s/step - loss: 0.0647\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 2071s 2s/step - loss: 0.0646\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 2087s 2s/step - loss: 0.0642\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 2095s 2s/step - loss: 0.0639\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 2095s 2s/step - loss: 0.0638\n"
     ]
    }
   ],
   "source": [
    "# Inception base without top\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(290,290,3))\n",
    "\n",
    "# Adding the last two fully-connected layers\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x) # global average pooling (flatten)\n",
    "x = Dense(1024, activation='relu')(x) # should be rather large with 228 output labels\n",
    "#x = Dropout(0.5)(x)\n",
    "y = Dense(228, activation='sigmoid')(x) # sigmoid instead of softmax to have independent probabilities\n",
    "model = Model(inputs=base_model.input, outputs=y)\n",
    "\n",
    "# Train only the top layer\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Use binary loss instead of categorical loss to penalize each output independently\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# 1000 steps = 640000 random images per epoch\n",
    "model.fit_generator(training_gen, steps_per_epoch=1000, epochs=10)\n",
    "\n",
    "model.save('D:/mlip/inceptionV3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155/155 [==============================] - 146s 941ms/step\n"
     ]
    }
   ],
   "source": [
    "# load the generator\n",
    "predict_gen = BatchSequence(input_dir=images_path_validation, y=y_validation, batch_size=64)\n",
    "\n",
    "predictions = model.predict_generator(predict_gen, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.30659386812263756 Recall: 0.009193723080023173 F1: 0.0178521208564127\n"
     ]
    }
   ],
   "source": [
    "y_true = y_validation\n",
    "y_pred = (predictions > 0.5).astype(int)\n",
    "\n",
    "pr = precision_score(y_true, y_pred, average='micro')\n",
    "rc = recall_score(y_true, y_pred, average='micro')\n",
    "f1 = f1_score(y_true, y_pred, average='micro')\n",
    "\n",
    "print(\"Precision: {} Recall: {} F1: {}\".format(pr, rc, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    print(i, layer.name)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "from keras.optimizers import SGD\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy')\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "model.fit_generator(...)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
